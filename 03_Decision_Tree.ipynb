{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importações**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = pd.read_csv(\"Teste.csv\")\n",
    "df_treino.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação dos dataset de validacao e teste\n",
    "file_path = r'Validacao.csv'\n",
    "df_validacao = pd.read_csv(file_path)\n",
    "df_validacao = df_validacao.astype(int)\n",
    "display(df_validacao.head())\n",
    "\n",
    "file_path = r'Teste.csv'\n",
    "df_teste = pd.read_csv(file_path)\n",
    "df_teste = df_teste.astype(int)\n",
    "display(df_teste.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Treino**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando as features que serão passadas para predição\n",
    "X = df_treino.drop(columns=['Resultado'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo as labels do dataset de treino\n",
    "Y = df_treino.drop(columns=['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8', 'J9'])['Resultado'].values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do Classificador\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Treino do algoritmo\n",
    "clf.fit(X.values,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando a árvore de decisão\n",
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação das predições a partir da base de validação\n",
    "predicao = clf.predict(X)\n",
    "\n",
    "# Calculando a acurácia\n",
    "acuracia = accuracy_score(Y, predicao)\n",
    "print(\"Acurácia: {:.2f}\".format(acuracia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Validação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando as features que serão passadas para predição\n",
    "X_valid = df_validacao.drop(columns=['Resultado'])\n",
    "X_valid = X_valid.values\n",
    "X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo as labels do dataset de validação\n",
    "Y_valid = df_validacao.drop(columns=['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8', 'J9'])['Resultado'].values\n",
    "Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação das predições a partir da base de validação\n",
    "predicao = clf.predict(X_valid)\n",
    "\n",
    "# Calcular a acurácia\n",
    "acuracia = accuracy_score(Y_valid, predicao)\n",
    "print(\"Acurácia: {:.2f}\".format(acuracia))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Teste**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando as features que serão passadas para predição\n",
    "X_test = df_teste.drop(columns=['Resultado'])\n",
    "X_test = X_test.values\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtendo as labels do dataset de teste\n",
    "Y_test = df_teste.drop(columns=['J1', 'J2', 'J3', 'J4', 'J5', 'J6', 'J7', 'J8', 'J9'])['Resultado'].values\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação das predições a partir da base de validação\n",
    "predicao = clf.predict(X_test)\n",
    "\n",
    "# Calcular a acurácia\n",
    "acuracia = accuracy_score(Y_test, predicao)\n",
    "print(\"Acurácia: {:.2f}\".format(acuracia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomes das classes\n",
    "nomes_classes = [\"O ganhou\",\"Empate\",\"X ganhou\",\"Ainda tem jogo\"]\n",
    "\n",
    "# Calcular e imprimir o relatório de classificação\n",
    "relatorio_classificacao = classification_report(Y_test, predicao, target_names=nomes_classes)\n",
    "print(\"Relatório de Classificação:\\n\", relatorio_classificacao)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
